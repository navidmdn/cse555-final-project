{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e097356",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e577654",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92340087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a276a",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e269b",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 128\n",
    "batch_size_test = 100\n",
    "random_seed = 12453211\n",
    "random_threshold = 0.6 #should be b/w 0 and 1\n",
    "mode = \"original\" # Can be symmetric, assymetric or original (default)\n",
    "partiality =  None # Can be balanced or imbalanced or None\n",
    "\n",
    "# For Dataset 1 - Balanced dataset -                 - mode = original   and partitality = balanced\n",
    "# For Dataset 2 - Imbalanced dataset Original MNIST  - mode = original   and partiality = None\n",
    "# For Dataset 3 - Balanced Symmetric Noise           - mode = Symmetric  and partiality = balanced\n",
    "# For Dataset 4 - Balanced Assymetric Noise          - mode = Assymetric and partiality = balanced\n",
    "# For Dataset 5 - Imbalanced Symmetric Noise         - mode = Symmetric  and partiality = imbalanced\n",
    "# For Dataset 6 - Imbalanced Assymetric Noise        - mode = Assymetric and partiality = imbalanced   \n",
    "\n",
    "imbalanced_weights = {\n",
    "    0: 0.3,\n",
    "    1: 0.3,\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 1.0,\n",
    "    5: 0.3,\n",
    "    6: 1.0,\n",
    "    7: 0.3,\n",
    "    8: 1.0,\n",
    "    9: 1.0\n",
    "}\n",
    "\n",
    "# Creating symmetric noise for 1,2 and 5 as 9,7 and 8\n",
    "symmetric_noise = {\n",
    "    0: 0,\n",
    "    1: 9,\n",
    "    9: 1,\n",
    "    2: 7,\n",
    "    7: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 8,\n",
    "    8: 5,\n",
    "    6: 6\n",
    "}\n",
    "\n",
    "# Creating asymettric noise for 0,3,4 and 8\n",
    "asymmetric_noise = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 4,\n",
    "    4: 8,\n",
    "    5: 5,\n",
    "    6: 6,\n",
    "    7: 7,\n",
    "    8: 3,\n",
    "    9: 0\n",
    "}\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837ee1c",
   "metadata": {},
   "source": [
    "## Symmetric and Assymetric Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d414d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"symmetric\" or mode == \"assymetric\":\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        '.',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]),\n",
    "        target_transform = lambda y: \n",
    "        (y if random.random() > random_threshold else symmetric_noise[y]) \n",
    "        if mode == \"symmetric\" else \n",
    "        (y if random.random() > random_threshold else asymmetric_noise[y])\n",
    "    )\n",
    "\n",
    "if mode == \"original\":\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        '.',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    )\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    '.',\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "        (0.1307,), (0.3081,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, [50000, 10000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3c276",
   "metadata": {},
   "source": [
    "## Balanced and Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd2aa9",
   "metadata": {},
   "source": [
    "## To numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62401856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_to_numpy(data_loader):\n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for x, y in data_loader:\n",
    "        result_x.append(x.numpy())\n",
    "        result_y.append(y.numpy())\n",
    "        \n",
    "    return np.concatenate(result_x, axis=0), np.concatenate(result_y, axis=0)\n",
    "    \n",
    "train_x, train_y = data_loader_to_numpy(train_loader)\n",
    "test_x, test_y = data_loader_to_numpy(test_loader)\n",
    "valid_x, valid_y = data_loader_to_numpy(valid_loader)\n",
    "\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imbalanced(ds_x, ds_y, imbalanced_weights=imbalanced_weights):\n",
    "    class_partition = {k:[] for k in range(10)}\n",
    "\n",
    "    for x, y in zip(ds_x, ds_y):\n",
    "        class_partition[y].append((x, y))\n",
    "\n",
    "    for i in range(10):\n",
    "        idxs = np.random.randint(0, len(class_partition[i]), int(imbalanced_weights[i]*len(class_partition[i])))\n",
    "        class_partition[i] = [class_partition[i][j] for j in idxs]\n",
    "        print(f\"class {i}: size={len(class_partition[i])}\")\n",
    "\n",
    "    imbalanced_train = []\n",
    "\n",
    "    for partition in class_partition.values():\n",
    "        imbalanced_train.extend(partition)\n",
    "\n",
    "    np.random.shuffle(imbalanced_train)\n",
    "    imbalanced_train_x, imbalanced_train_y = zip(*imbalanced_train)\n",
    "    \n",
    "    return imbalanced_train_x, imbalanced_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_balanced(ds_x, ds_y):\n",
    "    #data_count = Counter(ds_y)\n",
    "    #min_key, min_count = min(data_count.items(), key=itemgetter(1))\n",
    "    return ds_x,ds_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "if partiality == \"imbalanced\":\n",
    "    train_x, train_y = make_imbalanced(train_x, train_y)\n",
    "elif partiality == \"balanced\":\n",
    "    train_x, train_y = make_balanced(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def213ef",
   "metadata": {},
   "source": [
    "## Distribution plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaab902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plotter(df):\n",
    "    train_classes = [label for label in df]\n",
    "    data_count = Counter(train_classes)\n",
    "    palette = sns.color_palette(\"husl\")\n",
    "    plt.figure(figsize=(18,5))\n",
    "    sns.barplot(x=list(data_count.keys()),y=list(data_count.values()),palette=palette)\n",
    "    plt.xlabel('{}'.format(mode))\n",
    "\n",
    "distribution_plotter(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4698be",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a2538",
   "metadata": {},
   "source": [
    "## validation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68749836",
   "metadata": {},
   "source": [
    "### draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40690826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def conf_mat(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm, annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def clf_metrics(y_true, y_pred, n_class=10):\n",
    "    class_names = [str(i) for i in range(n_class)]\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3567eb",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d532b23",
   "metadata": {},
   "source": [
    "### preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x, y = x.squeeze(), y\n",
    "    return x.reshape((x.shape[0], -1)), y\n",
    "\n",
    "train_x, train_y = preprocess(train_x, train_y)\n",
    "test_x, test_y = preprocess(test_x, test_y)\n",
    "valid_x, valid_y = preprocess(valid_x, valid_y)\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b810598",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(\n",
    "    kernel='linear',\n",
    "    decision_function_shape='ovr',\n",
    "    random_state=random_seed,\n",
    "    verbose=True,\n",
    ") \n",
    "\n",
    "svm.fit(train_x, train_y)\n",
    "y_pred = svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167011ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83aedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ebcb1",
   "metadata": {},
   "source": [
    "### model report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af876e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_metrics(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731c57b",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd541d",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = train_x[0].shape[0]\n",
    "output_features = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0879205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n_input_features, output_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_input_features, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = self.linear(x)\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "model = LogisticRegression(input_features * input_features, output_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    images,labels = batch\n",
    "    print(images.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960c87f",
   "metadata": {},
   "source": [
    "## Training the logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_number, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, input_features *\n",
    "                             input_features).requires_grad_()\n",
    "        labels = labels\n",
    "         # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b29da5",
   "metadata": {},
   "source": [
    "## Testing the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc70b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "real_classes = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    # Load images to a Torch Variable\n",
    "    images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "    # Forward pass only to get logits/output\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get predictions from the maximum value\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = predicted.tolist()\n",
    "    labels = labels.tolist()\n",
    "    predictions.append(predicted)\n",
    "    real_classes.append(labels)\n",
    "\n",
    "predictions = [item for sublist in predictions for item in sublist]\n",
    "real_classes = [item for sublist in real_classes for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcf6e0",
   "metadata": {},
   "source": [
    "## Confusion matrix and predictions for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(real_classes,predictions)\n",
    "clf_metrics(real_classes,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468ae14",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1112c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/logistic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0c553",
   "metadata": {},
   "source": [
    "# SV Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc6902",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68779781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299bb00",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb6230",
   "metadata": {},
   "source": [
    "## LoggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9712e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerCallback(Callback):\n",
    "    \"\"\"\n",
    "    Log train/val loss and acc into file for later plots.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, X_train, y_train, y_train_clean, X_test, y_test, dataset,\n",
    "                 model_name, noise_ratio, asym, epochs, alpha, beta):\n",
    "        super(LoggerCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.y_train_clean = y_train_clean\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.n_class = y_train.shape[1]\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.noise_ratio = noise_ratio\n",
    "        self.asym = asym\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        self.train_loss_class = [None]*self.n_class\n",
    "        self.train_acc_class = [None]*self.n_class\n",
    "\n",
    "        # the followings are used to estimate LID\n",
    "        self.lid_k = 20\n",
    "        self.lid_subset = 128\n",
    "        self.lids = []\n",
    "\n",
    "        # complexity - Critical Sample Ratio (csr)\n",
    "        self.csr_subset = 500\n",
    "        self.csr_batchsize = 100\n",
    "        self.csrs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        tr_acc = logs.get('acc')\n",
    "        tr_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        val_acc = logs.get('val_acc')\n",
    "\n",
    "        self.train_loss.append(tr_loss)\n",
    "        self.test_loss.append(val_loss)\n",
    "        self.train_acc.append(tr_acc)\n",
    "        self.test_acc.append(val_acc)\n",
    "\n",
    "        print('ALL acc:', self.test_acc)\n",
    "\n",
    "        if self.asym:\n",
    "            file_name = 'log/asym_loss_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio)\n",
    "            np.save(file_name, np.stack((np.array(self.train_loss), np.array(self.test_loss))))\n",
    "            file_name = 'log/asym_acc_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio)\n",
    "            np.save(file_name, np.stack((np.array(self.train_acc), np.array(self.test_acc))))\n",
    "            file_name = 'log/asym_class_loss_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio)\n",
    "            np.save(file_name, np.array(self.train_loss_class))\n",
    "            file_name = 'log/asym_class_acc_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio)\n",
    "            np.save(file_name, np.array(self.train_acc_class))\n",
    "        else:\n",
    "            file_name = 'log/loss_%s_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio, self.alpha)\n",
    "            np.save(file_name, np.stack((np.array(self.train_loss), np.array(self.test_loss))))\n",
    "            file_name = 'log/acc_%s_%s_%s_%s.npy' % \\\n",
    "                        (self.model_name, self.dataset, self.noise_ratio, self.alpha)\n",
    "            np.save(file_name, np.stack((np.array(self.train_acc), np.array(self.test_acc))))\n",
    "\n",
    "        return\n",
    "\n",
    "class SGDLearningRateTracker(Callback):\n",
    "    def __init__(self, model):\n",
    "        super(SGDLearningRateTracker, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        init_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "        decay = float(K.get_value(self.model.optimizer.decay))\n",
    "        iterations = float(K.get_value(self.model.optimizer.iterations))\n",
    "        lr = init_lr * (1. / (1. + decay * iterations))\n",
    "        print('init lr: %.4f, current lr: %.4f, decay: %.4f, iterations: %s' % (init_lr, lr, decay, iterations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44306bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVModel(input_tensor=None, input_shape = (28, 28, 1), num_classes=10):\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_shape):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv1')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=\"he_normal\", name='fc1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu', name='lid')(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(num_classes, kernel_initializer=\"he_normal\")(x)\n",
    "    x = Activation(tf.nn.softmax)(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "    return model\n",
    "\n",
    "#model_svm = SVModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3dd31",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf89837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "def other_class(n_classes, current_class):\n",
    "    \"\"\"\n",
    "    Returns a list of class indices excluding the class indexed by class_ind\n",
    "    :param nb_classes: number of classes in the task\n",
    "    :param class_ind: the class index to be omitted\n",
    "    :return: one random class that != class_ind\n",
    "    \"\"\"\n",
    "    if current_class < 0 or current_class >= n_classes:\n",
    "        error_str = \"class_ind must be within the range (0, nb_classes - 1)\"\n",
    "        raise ValueError(error_str)\n",
    "\n",
    "    other_class_list = list(range(n_classes))\n",
    "    other_class_list.remove(current_class)\n",
    "    other_class = np.random.choice(other_class_list)\n",
    "    return other_class\n",
    "\n",
    "def get_lr_scheduler(dataset):\n",
    "    \"\"\"\n",
    "    customerized learning rate decay for training with clean labels.\n",
    "     For efficientcy purpose we use large lr for noisy data.\n",
    "    :param dataset: \n",
    "    :param noise_ratio:\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if dataset in ['mnist']:\n",
    "        def scheduler(epoch):\n",
    "            if epoch > 30:\n",
    "                return 0.001\n",
    "            elif epoch > 10:\n",
    "                return 0.01\n",
    "            else:\n",
    "                return 0.1\n",
    "        return LearningRateScheduler(scheduler)\n",
    "    elif dataset in ['cifar-10']:\n",
    "        def scheduler(epoch):\n",
    "            if epoch > 80:\n",
    "                return 0.0001\n",
    "            elif epoch > 40:\n",
    "                return 0.001\n",
    "            else:\n",
    "                return 0.01\n",
    "        return LearningRateScheduler(scheduler)\n",
    "    elif dataset in ['cifar-100']:\n",
    "        def scheduler(epoch):\n",
    "            if epoch > 120:\n",
    "                return 0.001\n",
    "            elif epoch > 80:\n",
    "                return 0.01\n",
    "            else:\n",
    "                return 0.1\n",
    "        return LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f8f40",
   "metadata": {},
   "source": [
    "## Symmetric cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4513f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_cross_entropy(alpha, beta):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_1 = y_true\n",
    "        y_pred_1 = y_pred\n",
    "\n",
    "        y_true_2 = y_true\n",
    "        y_pred_2 = y_pred\n",
    "\n",
    "        y_pred_1 = tf.clip_by_value(y_pred_1, 1e-7, 1.0)\n",
    "        y_true_2 = tf.clip_by_value(y_true_2, 1e-4, 1.0)\n",
    "\n",
    "        return alpha*tf.reduce_mean(-tf.reduce_sum(y_true_1 * tf.log(y_pred_1), axis = -1)) + beta*tf.reduce_mean(-tf.reduce_sum(y_pred_2 * tf.log(y_true_2), axis = -1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed016a",
   "metadata": {},
   "source": [
    "## Create noisy symmetric and assymetric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "NUM_CLASSES = {'mnist': 10, 'svhn': 10, 'cifar-10': 10, 'cifar-100': 100}\n",
    "\n",
    "def build_for_cifar100(size, noise):\n",
    "    \"\"\" random flip between two random classes.\n",
    "    \"\"\"\n",
    "    assert(noise >= 0.) and (noise <= 1.)\n",
    "\n",
    "    P = np.eye(size)\n",
    "    cls1, cls2 = np.random.choice(range(size), size=2, replace=False)\n",
    "    P[cls1, cls2] = noise\n",
    "    P[cls2, cls1] = noise\n",
    "    P[cls1, cls1] = 1.0 - noise\n",
    "    P[cls2, cls2] = 1.0 - noise\n",
    "\n",
    "    assert_array_almost_equal(P.sum(axis=1), 1, 1)\n",
    "    return P\n",
    "\n",
    "def multiclass_noisify(y, P, random_state=0):\n",
    "    \"\"\" Flip classes according to transition probability matrix T.\n",
    "    It expects a number between 0 and the number of classes - 1.\n",
    "    \"\"\"\n",
    "\n",
    "    assert P.shape[0] == P.shape[1]\n",
    "    assert np.max(y) < P.shape[0]\n",
    "\n",
    "    # row stochastic matrix\n",
    "    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
    "    assert (P >= 0.0).all()\n",
    "\n",
    "    m = y.shape[0]\n",
    "    new_y = y.copy()\n",
    "    flipper = np.random.RandomState(random_state)\n",
    "\n",
    "    for idx in np.arange(m):\n",
    "        i = y[idx]\n",
    "        # draw a vector with only an 1\n",
    "        flipped = flipper.multinomial(1, P[i, :], 1)[0]\n",
    "        new_y[idx] = np.where(flipped == 1)[0]\n",
    "\n",
    "    return new_y\n",
    "\n",
    "def get_data(dataset='mnist', noise_ratio=0, asym=False, random_shuffle=False):\n",
    "    \"\"\"\n",
    "    Get training images with specified ratio of syn/ayn label noise\n",
    "    \"\"\"\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    y_train_clean = np.copy(y_train)\n",
    "    # generate random noisy labels\n",
    "    if noise_ratio > 0:\n",
    "        if asym:\n",
    "            data_file = \"data/asym_%s_train_labels_%s.npy\" % (dataset, noise_ratio)\n",
    "            if dataset == 'cifar-100':\n",
    "                P_file = \"data/asym_%s_P_value_%s.npy\" % (dataset, noise_ratio)\n",
    "        else:\n",
    "            data_file = \"data/%s_train_labels_%s.npy\" % (dataset, noise_ratio)\n",
    "        if os.path.isfile(data_file):\n",
    "            y_train = np.load(data_file)\n",
    "            if dataset == 'cifar-100' and asym:\n",
    "                P = np.load(P_file)\n",
    "        else:\n",
    "            if asym:\n",
    "                if dataset == 'mnist':\n",
    "                    # 1 < - 7, 2 -> 7, 3 -> 8, 5 <-> 6\n",
    "                    source_class = [7, 2, 3, 5, 6]\n",
    "                    target_class = [1, 7, 8, 6, 5]\n",
    "                elif dataset == 'cifar-10':\n",
    "                    # automobile < - truck, bird -> airplane, cat <-> dog, deer -> horse\n",
    "                    source_class = [9, 2, 3, 5, 4]\n",
    "                    target_class = [1, 0, 5, 3, 7]\n",
    "\n",
    "                elif dataset == 'cifar-100':\n",
    "                        P = np.eye(NUM_CLASSES[dataset])\n",
    "                        n = noise_ratio/100.0\n",
    "                        nb_superclasses = 20\n",
    "                        nb_subclasses = 5\n",
    "\n",
    "                        if n > 0.0:\n",
    "                            for i in np.arange(nb_superclasses):\n",
    "                                init, end = i * nb_subclasses, (i+1) * nb_subclasses\n",
    "                                P[init:end, init:end] = build_for_cifar100(nb_subclasses, n)\n",
    "\n",
    "                            y_train_noisy = multiclass_noisify(y_train, P=P,\n",
    "                                                               random_state=0)\n",
    "                            actual_noise = (y_train_noisy != y_train).mean()\n",
    "                            assert actual_noise > 0.0\n",
    "                            y_train = y_train_noisy\n",
    "                        np.save(P_file, P)\n",
    "\n",
    "                else:\n",
    "                    print('Asymmetric noise is not supported now for dataset: %s' % dataset)\n",
    "                    return\n",
    "\n",
    "                if dataset == 'mnist' or dataset == 'cifar-10':\n",
    "                    for s, t in zip(source_class, target_class):\n",
    "                        cls_idx = np.where(y_train_clean == s)[0]\n",
    "                        n_noisy = int(noise_ratio * cls_idx.shape[0] / 100)\n",
    "                        noisy_sample_index = np.random.choice(cls_idx, n_noisy, replace=False)\n",
    "                        y_train[noisy_sample_index] = t\n",
    "\n",
    "            else:\n",
    "                n_samples = y_train.shape[0]\n",
    "                n_noisy = int(noise_ratio * n_samples / 100)\n",
    "                class_index = [np.where(y_train_clean == i)[0] for i in range(NUM_CLASSES[dataset])]\n",
    "                class_noisy = int(n_noisy / NUM_CLASSES[dataset])\n",
    "\n",
    "                noisy_idx = []\n",
    "                for d in range(NUM_CLASSES[dataset]):\n",
    "                    noisy_class_index = np.random.choice(class_index[d], class_noisy, replace=False)\n",
    "                    noisy_idx.extend(noisy_class_index)\n",
    "\n",
    "                for i in noisy_idx:\n",
    "                    y_train[i] = other_class(n_classes=NUM_CLASSES[dataset], current_class=y_train[i])\n",
    "            np.save(data_file, y_train)\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        print(\"Print noisy label generation statistics:\")\n",
    "        for i in range(NUM_CLASSES[dataset]):\n",
    "            n_noisy = np.sum(y_train == i)\n",
    "            print(\"Noisy class %s, has %s samples.\" % (i, n_noisy))\n",
    "\n",
    "    if random_shuffle:\n",
    "        # random shuffle\n",
    "        idx_perm = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train, y_train_clean = X_train[idx_perm], y_train[idx_perm], y_train_clean[idx_perm]\n",
    "\n",
    "    # one-hot-encode the labels\n",
    "    y_train_clean = np_utils.to_categorical(y_train_clean, NUM_CLASSES[dataset])\n",
    "    y_train = np_utils.to_categorical(y_train, NUM_CLASSES[dataset])\n",
    "    y_test = np_utils.to_categorical(y_test, NUM_CLASSES[dataset])\n",
    "\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"y_train:\", y_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "\n",
    "    return X_train, y_train, y_train_clean, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, y_train_clean, X_test, y_test = get_data(dataset='mnist', noise_ratio=40)\n",
    "Y_train = np.argmax(Y_train, axis=1)\n",
    "(_, Y_clean_train), (_, Y_clean_test) = mnist.load_data()\n",
    "clean_selected = np.argwhere(Y_train == Y_clean_train).reshape((-1,))\n",
    "noisy_selected = np.argwhere(Y_train != Y_clean_train).reshape((-1,))\n",
    "print(\"#correct labels: %s, #incorrect labels: %s\" % (len(clean_selected), len(noisy_selected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d589e20",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset='mnist', model_name='sl', batch_size=128, epochs=50, noise_ratio=0, asym=False, alpha = 1.0, beta = 1.0):\n",
    "    \"\"\"\n",
    "    Train one model with data augmentation: random padding+cropping and horizontal flip\n",
    "    :param dataset: \n",
    "    :param model_name:\n",
    "    :param batch_size: \n",
    "    :param epochs: \n",
    "    :param noise_ratio: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    print('Dataset: %s, model: %s, batch: %s, epochs: %s, noise ratio: %s%%, asymmetric: %s, alpha: %s, beta: %s' %\n",
    "          (dataset, model_name, batch_size, epochs, noise_ratio, asym, alpha, beta))\n",
    "\n",
    "    # load data\n",
    "    X_train, y_train, y_train_clean, X_test, y_test = get_data(dataset, noise_ratio, asym=asym, random_shuffle=False)\n",
    "    n_images = X_train.shape[0]\n",
    "    image_shape = X_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "    print(\"n_images\", n_images, \"num_classes\", num_classes, \"image_shape:\", image_shape)\n",
    "    \n",
    "    # define P for forward and backward loss\n",
    "    P = np.eye(num_classes)\n",
    "    \n",
    "    # load model\n",
    "    model = SVModel(input_tensor=None, input_shape=image_shape, num_classes=num_classes)\n",
    "    # model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)    \n",
    "\n",
    "    # create loss\n",
    "    loss = symmetric_cross_entropy(alpha,beta)\n",
    "\n",
    "    # model\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    if asym:\n",
    "        model_save_file = \"model/asym_%s_%s_%s.{epoch:02d}.hdf5\" % (model_name, dataset, noise_ratio)\n",
    "    else:\n",
    "        model_save_file = \"model/%s_%s_%s.{epoch:02d}.hdf5\" % (model_name, dataset, noise_ratio)\n",
    "\n",
    "\n",
    "    ## do real-time updates using callbakcs\n",
    "    callbacks = []\n",
    "\n",
    "    if model_name == 'sl':\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(model_save_file,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      save_best_only=False,\n",
    "                                      save_weights_only=True,\n",
    "                                      period=1)\n",
    "        callbacks.append(cp_callback)\n",
    "    else:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(model_save_file,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      save_best_only=False,\n",
    "                                      save_weights_only=True,\n",
    "                                      period=1)\n",
    "        callbacks.append(cp_callback)\n",
    "\n",
    "    # learning rate scheduler if use sgd\n",
    "    lr_scheduler = get_lr_scheduler(dataset)\n",
    "    callbacks.append(lr_scheduler)\n",
    "\n",
    "    callbacks.append(SGDLearningRateTracker(model))\n",
    "\n",
    "    # acc, loss, lid\n",
    "    log_callback = LoggerCallback(model, X_train, y_train, y_train_clean, X_test, y_test, dataset, model_name, noise_ratio, asym, epochs, alpha, beta)\n",
    "    callbacks.append(log_callback)\n",
    "\n",
    "    # data augmentation\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # train model\n",
    "    model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(X_train) / batch_size, epochs=epochs,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9482e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e25ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
